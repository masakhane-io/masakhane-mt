# Data

- Vukosi found amazing StatsSA data for technical terms in South African languages and has asked for an excel spreadsheet
- Vukosi has also acquired a book of Idioms for English to Xitsonga so they're contacting the publisher to see if they have a digitized text. Concerns raised around the religious nature of the data and thus we'll need more things
- Blessing has found a source of scraped Shona data - not translated. Around 100k sentences. Possibilities for back translation or at least a language model. cocohub.cc was brought up as a possible way to do the translations. Jade to chat to Brian about it. 
- Musie says the data for Amharic is not aligned properly which is strange. We're going to speak to the creators of the JW300 dataset and the Opus creators. 
- Abdoul to use Musie's script to scrape the Wolof data from JW.org
- Neelke asked about which languages she should work on - go deeper into Afrikaans or try out languages she doesn't know. Jade said she could do whatever she prefered. If she was going to work on a language she doesn't know then ideally she'd be paired up with someone who does know the language! 
- Olfa and Elyes discussed the Tunisia data. 800 patterns is too small. They should definitely try transfer learning with other Arabic languages. [This](https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html ) article on how to do it was shared on slack by Julia
- Elyes suggested using the latin letters instead of arabic. Jade said it was worth trying because distinct character sets are harder to translate. 

# Experimental Updates

- Julia, in preparation for future transfer learning, has created global test sets to ensure we don't have leakage between test and training sets when we do the transfer learning
- As a result, all notebooks using JW300 data should update to the latest version and use the latest test sets and scripts that Julia wrote. They prevent leakage
- Julia suggested we spend time verifying our test sets are (a) correct (b) big enough. If they're not big enough, we should translate patterns that exist in other language test sets for that language. A test set around 1000 sentences would be ideal.

# Notebooks and Submissions

- Does JoeyNMT save models? It saves the last 3 best models (this is configurable).
- The current default notebook will save those models to your google drive. 
- When people's training is done, ping Jade if you'd like her to guide you through the creation of a branch and submission of data and models
- Reminder to email models through (because they're probably too big to commit) when you submit code/datasets

# General Announcements

- We're updated the deadlines. We'll likely target EMNLP or TACL instead and thus we'll add a few months onwards!
- Julia has agreed to write up something on strategies for fine-tuning models for the languages. 
- We will probably move the meeting to 6:30 on a Thursday for a bit - by popular demand. 
- We have $9000 GCP credits, so if you'res is taking too long on colab, let me know and I'll add you to the list for people who will get a machine. Vukosi mentioned that GCP images have weird versions on python so creating a docker image would be useful for that. Espoir offered to help get that setup. 
- Jade is keeping a research journal and encourages everyone else to also keep a research journal - an excellent idea from Vukosi. The idea is that our learnings can be applied to future collaborative research. 

*If there is anything I've missed out - please feel free to add them :)*